{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3\n",
    "\n",
    "\n",
    "> Kaggle Name: Noven_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import gzip\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from random import sample, randint\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import pprint\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.json.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cf6e4298c766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadGz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.json.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-cf6e4298c766>\u001b[0m in \u001b[0;36mreadGz\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadGz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadGz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.json.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.json.gz'"
     ]
    }
   ],
   "source": [
    "def readGz(f):\n",
    "  for l in gzip.open(f):\n",
    "    yield eval(l)\n",
    "\n",
    "data = list(readGz('train.json.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[0:10000]\n",
    "\n",
    "val = data[100000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Although we have built a validation set, it only consists of positive samples. For this task we also need\n",
    "examples of user/item pairs that weren't purchased. Build such a set by randomly sampling users and\n",
    "items until you have 100,000 non-purchased user/item pairs. This random sample combined with your\n",
    "100,000 validation reviews now corresponds to the complete validation set for the purchase prediction\n",
    "task. Evaluate the performance (accuracy) of the baseline model on the validation set you have built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unique_reviewer1 = list(set([d['reviewerID'] for d in data]))\n",
    "unique_business1 = list(set([d['itemID'] for d in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "    parsed_data = []\n",
    "    for line in open(fname, \"r\"):\n",
    "        parsed_data.append(line.strip())\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_Ratings = parseData(\"pairs_Rating.txt\")\n",
    "pairs_Ratings = pairs_Ratings[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_ratings_predictions = []\n",
    "y_test_ratings_predictions_file = \"predictions_Rating_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(y_test_ratings_predictions_file):\n",
    "    f = open(y_test_ratings_predictions_file, 'w')\n",
    "    f.write(\"reviewerID-itemID,prediction\\n\")\n",
    "    for pair in pairs_Ratings:\n",
    "        u = pair.split(\"-\")[0]\n",
    "        b = pair.split(\"-\")[1]\n",
    "        uid = u[1:]\n",
    "        iid = b[1:]\n",
    "        pred = algo.predict(uid,iid)\n",
    "        rating = pred[3]\n",
    "        f.write(u + \"-\" + b + \",\" + str(rating) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_pairs = []\n",
    "for datum in data:\n",
    "    visited_pairs.append((datum['reviewerID'], datum['itemID']))\n",
    "visited_pairs = set(visited_pairs)\n",
    "len(visited_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_visted_pairs = set()\n",
    "counter = 100000\n",
    "while counter:\n",
    "    random_item_id = random.sample(unique_business1, 1)[0]\n",
    "    random_userID = random.sample(unique_reviewer1, 1)[0]\n",
    "    if ((random_userID, random_item_id) not in visited_pairs\n",
    "        and (random_userID, random_item_id) not in non_visted_pairs):\n",
    "            non_visted_pairs.add((random_userID, random_item_id))\n",
    "            counter -= 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataset = [[d[0], d[1]] for d in non_visted_pairs]\n",
    "y_sampled = [0]*len(sampled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = [[d['reviewerID'], d['itemID']] for d in train]\n",
    "y_train1 = [1]*len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data1 = [[d['reviewerID'], d['itemID']] for d in val]\n",
    "y_validation1 = [1]*len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data1 = val_data1 + sampled_dataset\n",
    "\n",
    "x_val1 = val_data1\n",
    "y_val1 = y_validation1 + y_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessCount = defaultdict(int)\n",
    "totalPurchases = 0\n",
    "for l in x_train:\n",
    "    user,business = l[0],l[1]\n",
    "    businessCount[business] += 1\n",
    "    totalPurchases += 1\n",
    "\n",
    "mostPopular = [(businessCount[x], x) for x in businessCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "\n",
    "    if count > totalPurchases/2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predictions = []\n",
    "for x in x_val:\n",
    "    if x[1] in return1:\n",
    "        y_val_predictions.append(1)\n",
    "    else:\n",
    "        y_val_predictions.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance (accuracy) of the baseline model on the validation set is: \", str(accuracy_score(y_val_predictions, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Purchase.txt\", 'w')\n",
    "for l in open(\"pairs_Purchase.txt\"):\n",
    "  if l.startswith(\"reviewerID\"):\n",
    "    #header\n",
    "    predictions.write(l)\n",
    "    continue\n",
    "  u,i = l.strip().split('-')\n",
    "  if i in return1:\n",
    "    predictions.write(u + '-' + i + \",1\\n\")\n",
    "  else:\n",
    "    predictions.write(u + '-' + i + \",0\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.The existing `purchase prediction' baseline just returns True if the item in question is `popular,' using a\n",
    "threshold of the 50th percentile of popularity (totalPurchases/2). Assuming that the `non-purchased'\n",
    "test examples are a random sample of user-purchase pairs, is this particular threshold value the best? If\n",
    "not, see if you can \f",
    "nd a better one (and report its performance), or if so, explain why it is the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimum threshold to get the best score in the validation set is 49% which is very close to 50% as in the baseline code. This is because  \n",
    "half the reviewer/item paris is negative samples that has never happened and half is positive as in the trainig data. The predictor will perform\n",
    "fairly better on the easy data than the hard ones, therefore, the best cut occurs around 50%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessCount = defaultdict(int)\n",
    "totalPurchases = 0\n",
    "for l in x_train:\n",
    "    user,business = l[0],l[1]\n",
    "    businessCount[business] += 1\n",
    "    totalPurchases += 1\n",
    "\n",
    "mostPopular = [(businessCount[x], x) for x in businessCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "def partition_at(threshold):\n",
    "    mostPopular_business = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        mostPopular_business.add(i)\n",
    "        if count > totalPurchases*((100.0 - threshold)/100.0):\n",
    "            break\n",
    "    return mostPopular_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0, 100, 1)\n",
    "threshold_accuracies = []\n",
    "\n",
    "for t in thresholds:\n",
    "    check_set = partition_at(t)\n",
    "    y_validation_predictions = []\n",
    "    for x in x_val:\n",
    "        if x[1] in check_set:\n",
    "            y_validation_predictions.append(1)\n",
    "        else:\n",
    "            y_validation_predictions.append(0)\n",
    "    accuracy = accuracy_score(y_validation_predictions, y_val)\n",
    "    threshold_accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(threshold_accuracies)):\n",
    "    print(\"Percentile: \", str(thresholds[i]), \" Accuracy: \", str(threshold_accuracies[i]))\n",
    "\n",
    "index_max_thresh_accu = threshold_accuracies.index(max(threshold_accuracies))\n",
    "print(\"\\nA better value of accuracy is \", str(max(threshold_accuracies)))\n",
    "print(\"It occurs at \", str(thresholds[index_max_thresh_accu]), str(\" percentile.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Users may tend to repeatedly purchase items of the same type. Build a baseline that returns `True' if\n",
    "a user has purchased an item of the same category before (at least one category in common), or zero\n",
    "otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## def flatten(items, seqtypes=(list, tuple)):\n",
    "    ## for i, x in enumerate(items):\n",
    "      ##  while i < len(items) and isinstance(items[i], seqtypes):\n",
    "          ##  items[i:i+1] = items[i]\n",
    "  ##  return items\n",
    "## cat = [d['categories'] for d in data]\n",
    "\n",
    "## fcat = flatten(cat[:])\n",
    "## my_cat = list(set(fcat))\n",
    "\n",
    "\n",
    "## reviewerIds = dict(zip(unique_reviewer, range(len(unique_reviewer))))\n",
    "## businessIds = dict(zip(unique_business, range(len(unique_business))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userCategoriesVisited = defaultdict(set)\n",
    "for d in train:\n",
    "    uId = d['reviewerID']\n",
    "    # reviewerIds[unique_reviewer]\n",
    "    userCategoriesVisited[uId].add(d['categoryID'])\n",
    "\n",
    "    \n",
    "businessCategories = defaultdict(set)\n",
    "for d in train:\n",
    "    bId = d['itemID']\n",
    "    businessCategories[bId].add(d['categoryID'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Popular_business = partition_at(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(uId, bId):\n",
    "    if bId in Popular_business:\n",
    "        cId = businessCategories[bId]\n",
    "        for c in cId:\n",
    "            if c in userCategoriesVisited[uId]:\n",
    "                  return 1 \n",
    "   \n",
    "\n",
    "    return 0\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predictions = []\n",
    "for x in x_train:\n",
    "    uid = x[0]\n",
    "    bid = x[1]\n",
    "    y_train_predictions.append(predict(uid, bid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = accuracy_score(y_train_predictions,y_train)\n",
    "print(\"Accuracy is \", str(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation_predictions = []\n",
    "for x in x_val:\n",
    "    uid = x[0]\n",
    "    bid = x[1]\n",
    "    y_validation_predictions.append(predict(uid, bid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_validation_predictions, y_val)\n",
    "print(\"Accuracy is \", str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Purchase.txt\", 'w')\n",
    "for l in open(\"pairs_Purchase.txt\"):\n",
    "    if l.startswith(\"reviewerID\"):\n",
    "    #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    predictions.write(u + \"-\" + i + \",\" + str(int(predict(u, i))) + \"\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What is the performance of a trivial predictor on the validation set, and what is the value of alpha?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of a trival predictor on the validation set is 1.222; Alpha is 4.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat(d):\n",
    "    return [1]\n",
    "\n",
    "training_data = data[:100000]\n",
    "x_train = [feat(d) for d in training_data]\n",
    "y_train = [d['rating'] for d in training_data]\n",
    "\n",
    "theta,residuals,rank,s = np.linalg.lstsq(x_train, y_train, rcond=None)\n",
    "print(\"The value of alpha is \", str(theta[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data[100000:]\n",
    "validation_dataset = [feat(d) for d in validation_data]\n",
    "y_validation = [d['rating'] for d in validation_data]\n",
    "y_validation_predictions = validation_dataset*theta\n",
    "\n",
    "sum = 0.0\n",
    "for i in range(len(y_validation)):\n",
    "    sum += (y_validation_predictions[i] - y_validation[i])**2\n",
    "MSE_validation = sum/len(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE on validation set: \", str(MSE_validation[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6 Report the MSE on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE for validation set without negative sample is:  1.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unique_reviewer = list(set([d['reviewerID'] for d in data]))\n",
    "## unique_business = list(set([d['itemID'] for d in data]))\n",
    "## reviewerIds = dict(zip(unique_reviewer, range(len(unique_reviewer))))\n",
    "## businessIds = dict(zip(unique_business, range(len(unique_business))))\n",
    "## num_reviewer = len(unique_reviewer)\n",
    "## num_business = len(unique_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_reviewer = list(set([d['reviewerID'] for d in data]))\n",
    "unique_business = list(set([d['itemID'] for d in data]))\n",
    "reviewerIds = dict(zip(unique_reviewer, range(len(unique_reviewer))))\n",
    "businessIds = dict(zip(unique_business, range(len(unique_business))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_by_users = defaultdict(dict)\n",
    "ratings_for_businesses = defaultdict(dict)\n",
    "\n",
    "for d in training_data:\n",
    "    index_u = reviewerIds[d['reviewerID']]\n",
    "    index_b = businessIds[d['itemID']]\n",
    "    ratings_by_users[index_u][index_b] = d['rating']\n",
    "    ratings_for_businesses[index_b][index_u] = d['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = theta[0]\n",
    "betaU = [1.0] * len(reviewerIds)\n",
    "betaI = [1.0] * len(businessIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_for_visitedpairs(datum):\n",
    "    feat = [1]\n",
    "    feat.append(reviewerIds[datum['reviewerID']])\n",
    "    feat.append(businessIds[datum['itemID']])\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_for_nonvisitedpairs(datum):\n",
    "    feat = [1]\n",
    "    feat.append(reviewerIds[datum.split(\",\")[1]])\n",
    "    feat.append(businessIds[datum.split(\",\")[0]])\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_alpha_beta(alpha, bU, bI, Ru, Rb, train_data, lam):\n",
    "    a = 0.0\n",
    "    for uid in list(range(len(bU))):\n",
    "        for bid, rating in Ru[uid].items():\n",
    "            a += (rating - (bU[uid] + bI[bid]))/len(train_data)\n",
    "    alpha = a\n",
    "    \n",
    "    bU_new = [0.0]*len(bU)\n",
    "    for uid in list(range(len(bU))):\n",
    "        for bid, rating in Ru[uid].items():\n",
    "                bU_new[uid] += (rating - (alpha + bI[bid]))/(lam + len(Ru[uid]))\n",
    "    bU = bU_new\n",
    "    \n",
    "    bI_new = [0.0]*len(bI)\n",
    "    for bid in list(range(len(bI))):\n",
    "        for uid, rating in Rb[bid].items():\n",
    "            bI_new[bid] += (rating - (alpha + bU[uid]))/(lam + len(Rb[bid]))\n",
    "    bI = bI_new\n",
    "    return a, bU_new, bI_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_cost(alpha, bU, bI, Ru, lam):\n",
    "    bUsq = np.sum(np.square(bU))\n",
    "    bIsq = np.sum(np.square(bI))\n",
    "    \n",
    "    term1 = 0.0\n",
    "    for uid in list(range(len(bU))):\n",
    "        for bid, r in Ru[uid].items():\n",
    "            term1 += (alpha + bU[uid] + bI[bid] - r)**2\n",
    "    return term1 + lam*(bUsq + bIsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_after_prediction(x, y, a, bU, bI):\n",
    "    y_pred = []\n",
    "    for i in range(len(x)):\n",
    "        r = a*x[i][0] + bU[x[i][1]] + bI[x[i][2]]\n",
    "        y_pred.append(r)\n",
    "        term = 0.0\n",
    "    for i in range(len(y)):\n",
    "        term += (y_pred[i] - y[i])**2\n",
    "    RMSE = np.sqrt(term/len(y))\n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data[100000:]\n",
    "validation_dataset = [feature_for_visitedpairs(d) for d in validation_data]\n",
    "y_validation = [d['rating'] for d in validation_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training_data = data[:100000]\n",
    "## x_train = [feature(d) for d in training_data]\n",
    "## y_train = [d['rating'] for d in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "RMSEs_validation = []\n",
    "for i in range(100):\n",
    "    alpha, betaU, betaI = update_alpha_beta(alpha, betaU, betaI, ratings_by_users, ratings_for_businesses, training_data, 1)\n",
    "    costs.append(find_cost(alpha, betaU, betaI, ratings_by_users, 1))\n",
    "    RMSE = RMSE_after_prediction(validation_dataset,\n",
    "                                y_validation, alpha,\n",
    "                                betaU, betaI)\n",
    "    RMSEs_validation.append(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(betaU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(100)), costs)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(100)), RMSEs_validation)\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"RMSE on validation dataset (without non-visited pairs)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(costs[-50:])\n",
    "\n",
    "for i in range(-100, 0):\n",
    "    print(costs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the value tends to converge at 65288.804."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data[100000:]\n",
    "validation_dataset = [feature_for_visitedpairs(d) for d in validation_data]\n",
    "y_validation = [d['rating'] for d in validation_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation_predictions = []\n",
    "for i in range(len(validation_dataset)):\n",
    "    rating = alpha*validation_dataset[i][0]\n",
    "    rating += betaU[validation_dataset[i][1]]\n",
    "    rating += betaI[validation_dataset[i][2]]\n",
    "    y_validation_predictions.append(rating)\n",
    "    \n",
    "sum = 0.0\n",
    "for i in range(len(y_validation)):\n",
    "    sum += (y_validation_predictions[i] - y_validation[i])**2\n",
    "MSE_val_set = sum/len(y_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE for validation set without negative sample is: \", str(MSE_val_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. Report the user and item IDs that have the largest and smallest values of β."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=reviewerIds.items()\n",
    "xi=businessIds.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"User with the largest beta:\", str([v[0] for i, v in enumerate(x) if v[1] == betaU.index((max(betaU)))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Item with the largest beta:\", str([v[0] for i, v in enumerate(xi) if v[1] == betaI.index((max(betaI)))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"User with the smallest beta:\", str([v[0] for i, v in enumerate(x) if v[1] == betaU.index((min(betaU)))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Item with the smallest beta:\", str([v[0] for i, v in enumerate(xi) if v[1] == betaI.index((min(betaI)))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. Find a better value of λ using your validation set. Report the value you chose, its MSE, and upload your solution to Kaggle by running it on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_for_prediction(x, y, a, bU, bI):\n",
    "    y_pred = []\n",
    "    for i in range(len(x)):\n",
    "        r = a*x[i][0] + bU[x[i][1]] + bI[x[i][2]]\n",
    "        y_pred.append(r)\n",
    "    Sum = 0.0\n",
    "    for i in range(len(y)):\n",
    "        Sum += (y_pred[i] - y[i])**2\n",
    "    MSE = Sum/len(y)\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.1, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20.0, 50.0, 100.0, 1000.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data[100000:]\n",
    "validation_dataset = [feature_for_visitedpairs(d) for d in validation_data]\n",
    "y_validation = [d['rating'] for d in validation_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEs_validation=[]\n",
    "for lam in lambdas:\n",
    "    alpha = theta[0]\n",
    "    betaU = [1.0] * len(reviewerIds)\n",
    "    betaI = [1.0] * len(businessIds)\n",
    "    for i in range(100):\n",
    "        alpha, betaU, betaI = update_alpha_beta(alpha, betaU, betaI, ratings_by_users, ratings_for_businesses, training_data, lam)\n",
    "    MSE = MSE_for_prediction(validation_dataset, y_validation, alpha, betaU, betaI)\n",
    "    print(\"Lambda: \", str(lam), \"MSE: \", str(MSE))\n",
    "    MSEs_validation.append(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The lambda for the smallest MSE of validation set is \", str(7.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The minimum MSE of validation set is \", str(min(MSEs_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training to optimize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = data[:100000]\n",
    "train_set = [feature_for_visitedpairs(d) for d in training]\n",
    "y_training = [d['rating'] for d in training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestlam = 7.0\n",
    "alpha = theta[0]\n",
    "betaU = [1.0] * len(reviewerIds)\n",
    "betaI = [1.0] * len(businessIds)\n",
    "    \n",
    "for i in range(100):\n",
    "    alpha, betaU, betaI = update_alpha_beta(alpha, betaU, betaI, ratings_by_users, ratings_for_businesses, training_data, bestlam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_rating(uid, bid, a, bU, bI, ru, rb):\n",
    "    rating = 0.0\n",
    "    if (uid not in reviewerIds and bid in businessIds):\n",
    "        for u in bU:\n",
    "            rating += a + u + bI[businessIds[bid]]\n",
    "        rating = rating/len(bU)\n",
    "    elif (uid in reviewerIds and bid not in businessIds):\n",
    "        for i in bI:\n",
    "            rating += a + bU[reviewerIds[uid]] + i\n",
    "        rating = rating/len(bI)\n",
    "    elif (uid not in reviewerIds and bid not in businessIds):\n",
    "        rating = a\n",
    "    elif (uid in reviewerIds and bid in businessIds):\n",
    "        rating += a + bU[reviewerIds[uid]] + bI[businessIds[bid]]\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "y_test_ratings_predictions = []\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"reviewerID\"):\n",
    "    #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    uid, bid = l.strip().split('-')\n",
    "    Rating = pred_rating(uid, bid, alpha, betaU, betaI, ratings_by_users, ratings_for_businesses)\n",
    "    predictions.write(uid + '-' + bid + ',' + str(Rating) + '\\n')\n",
    "    y_test_ratings_predictions.append(Rating)\n",
    "    \n",
    "predictions.close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
